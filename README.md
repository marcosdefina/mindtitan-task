# Sample task starter

Here you'll find a skeleton project for a simple Flask API.
To include private configuration, rename `config/private_RENAME_THIS.py` to `private.py`. Keep secret credentials there.

Build the docker image:  
`sudo docker build -t sampletask .`

Run the docker image:
`sudo docker run --net="host" sampletask`

Run as developer:
`python main.py`

The "Hello world!" can now be observed on   localhost at port 5000. 

### Data architecture
 - I tried to use everything as dictionary. Where the user_id is the key and the request_counter is the value, the nearst of json in python I could imagine.
 - the landing page is redirect to the form.html template, where there is an id input, which of course would be auto generated by api and would be requested in the user login, but in this case I tryed to be simplier. The question is not being used, it just disapier and if the user_id already exists, the value in the dictionary will increment by 1, if not, will be created a new element in the dictionary with 1 in its value.
  - When you click submit, you'll be redirected to a page where shows a table with your key and you message, but this is just garbage from when I was learning flask, like, 3 hours ago. I just forgot to take it out. If you want to see the things changing, try the '/database' end-point.

### Explanations

 - I was trying to deal with this organization with flask and things I didn't know about python, like how to use C-pthread libraries like in python, or even if I had to do it.

 - As I guessed, I had to do is to simulate the backbone of the system. And I kind of did it.

 - I would just need more time to make it works properly.

### Plans
 - Understand and apply [Elastic Load Balancing](https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html) 
 - I have read a little bit of AWS lambda functions - Serverless backend, as I understood, maybe would help to minimize the costs of our hypothetical company, and could be as scalable as we want if we just using the horizontal scalability, sending requests to be requested by free cores. Supposing infinite cores at AWS we could have only the ping and the processing as response time, avoiding the problem with overload in the old approach.

### Exercice 5 - Bonus
 - With 400k access per month, where the most active part of the day is early evening (assumed as 4 hours), we would have about 2500 requests per hour in those hours. As Erik Duindam said in his [Article](https://medium.com/unboxd/how-i-built-an-app-with-500-000-users-in-5-days-on-a-100-server-77deeb238e83) on Medium, we could feed those requests using low-level database requests, specifically designed for our purposes. This could save money from the servers.
     - All the efforts should be put in the developing of the classifier, all queries to the database should be as much precision as possible - even if it's needed a theoretical proof -
     - Once optimized, the efforts should be aimed at the Load Balancer, using preferentially **The Round Robin Method**, as the requests have the Worst Case Processing Time(WCPT) known. This will order the cores to work in a queue, as large as we want, following the equation `number_of_cores = WCPT*2500requests/wished_time_of_waiting`.
      - At last, we could pay more to use more processors, and control it as the users show their satisfaction with the product.